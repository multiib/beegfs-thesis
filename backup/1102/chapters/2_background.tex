\chapter{Background}

% TODO: Refine, write about importance
\section{History of HPC}
High-performance computing has a long history of driving advancements in science and technology. It began in the mid-20th century with the development of early computing systems such as the ENIAC in 1945, which was designed to handle complex calculations that were previously impossible. However, it was the introduction of the CDC 6600 in 1964, designed by Seymour Cray, that is widely regarded as the birth of modern supercomputing.\cite{cook2015history} This machine introduced parallel processing and vector computation, both key concepts that have shaped the evolution of HPC. 
\\\\
Throughout the 1970s and 1980s, supercomputing continued to advance, with systems like the Cray-1 in 1976 bringing higher processing speeds and improved efficiency through vector processing. These developments were crucial for fields such as physics, weather forecasting, and large-scale simulations, which required enormous amounts of computational power. The 1990s saw further innovation with the rise of massively parallel processing (MPP) systems, which used thousands of processors to perform calculations simultaneously. During this time, the Beowulf cluster architecture also emerged, enabling more affordable and scalable supercomputing by using standard hardware components.
\\\\
In the 21st century, HPC has continued to grow in importance, with systems reaching petascale performance, such as IBM’s Roadrunner in 2008, which was capable of over one quadrillion calculations per second (1 petaflop). Today, the field is moving towards exascale computing, with new systems like Frontier aiming to exceed one exaflop ($10^{18}$ floating-point operations per second).\cite{top500_2024} These advancements enable increasingly complex scientific research, including large-scale data analysis, climate modeling, and artificial intelligence.
\\\\
As HPC systems evolve, improving individual components becomes essential to meet rising computational demands. Technologies like RDMA offer significant performance gains by enabling direct memory access between systems, reducing CPU involvement and improving data transfer speeds. 

\section{IO as a bottleneck}
In HPC clusters, I/O performance is critical to ensure that applications can efficiently read and write large volumes of data. However, I/O often becomes a significant bottleneck, limiting the overall performance of applications.\cite{9355272}
\\\\
A main cause of I/O bottlenecks in HPC clusters is the centralized or sequential data access patterns common in traditional filesystems, which are not built to support multiple processes accessing data at the same time. In HPC clusters, thousands of compute nodes may try to read or write data simultaneously, which can overwhelm standard storage systems and create I/O contention. This problem is especially noticeable in data-intensive workloads, where frequent access to large datasets intensifies contention and reduces effective I/O throughput.
\\\\
To overcome these limitations, parallel filesystems are essential in HPC storage systems. Parallel filesystems like BeeGFS, Lustre, and GPFS (! is parallel?) are designed to spread data across multiple storage nodes, allowing many processes to access data at the same time. By managing data in parallel, these filesystems can handle high I/O demands and reduce contention, avoiding the bottlenecks common in traditional storage systems. Parallel filesystems use striping of data across multiple storage nodes, allowing data to be read and written at the same time across these nodes. This setup improves data access speeds and reduces bottlenecks, as the workload is shared rather than focused on one location.

\section{BeeGFS}
BeeGFS (before: Fraunhofer Parallel File System) is a high-performance, parallel file system designed for use in HPC clusters. Originally developed by the Fraunhofer Institute for Industrial Mathematics, BeeGFS is engineered to scale out horizontally across multiple servers, distributing file data and metadata across multiple nodes. This architecture allows BeeGFS to handle intensive workloads by spreading I/O operations over multiple storage nodes, enhancing both performance and data redundancy. It supports both traditional storage setups and modern architectures, such as NVMe and SSDs, which makes it adaptable for various HPC configurations.
\\\\
One of the key features of BeeGFS is the ability to separate metadata from file data, enabling efficient handling of file system operations like lookups, renames, and access permissions. There are designated meta data nodes. This division means that metadata operations don’t interfere with data transfer, resulting in faster file operations and improved overall system performance. BeeGFS also includes other features, like monitoring, quotas/groups and data mirroring. Additionally, BeeGFS integrates easily with existing infrastructure and supports standard POSIX file system rules.

\section{Remote Direct Memory Access}
Remote Direct Memory Access (RDMA) is a technology that enables high-speed data transfer between memory spaces on different systems without involving the CPU, operating system, or I/O buffers. This bypass of traditional networking layers allows for extremely low-latency communication and high throughput, which is  beneficial in HPC where efficient data movement is critical. RDMA operates by allowing one system to read or write directly into the memory of another over a network, typically via standards like InfiniBand or RDMA over Converged Ethernet (RoCE).
\\\\
When establishing an RDMA connection, specific memory regions need to be registered with the NICs to allow direct data access. This memory registration step assigns certain memory areas to the NICs, giving permission to access these addresses directly.
\\\\
After memory registration, RDMA establishes a queue pair (QP) connection, which includes a send queue and a receive queue. Each side of the connection places work requests (like read, write, or send) into these queues. The NIC then handles these requests independently, without needing the CPU to get involved. 


\section{PCI Express}
PCI Express (PCIe) is a high-speed interface standard for connecting various hardware components within a computer, such as CPUs, GPUs, storage devices, and network interfaces. It uses a point-to-point architecture, where each device has its own direct connection to the chipset. This setup reduces data congestion and allows for much faster data transfers compared to older, shared-bus designs.
\\\\
PCIe connections are made up of lane—pairs of paths for sending and receiving data. Devices can use multiple lanes (like x4, x8, or x16) to increase bandwidth according to their needs. Each new generation of PCIe has doubled the transfer speed per lane, supporting high-performance devices such as GPUs and NVMe storage with the bandwidth they require.
\\\\
Although PCIe is mainly used for connecting components within a single computer, it can also link the memory of two separate computers, allowing for direct, high-speed data transfer between them.


\section{Dolphin Interconnects}
Dolphin Interconnect Solutions develops high-performance networking products that use RDMA and PCI Express to achieve extremely low-latency data transfer between computing nodes. By combining RDMA’s direct memory access with PCIe’s high-speed architecture, Dolphin enables data to move directly between devices without needing CPU intervention. Using PCIe as the foundation, Dolphin interconnects allow peer-to-peer communication between nodes, bypassing traditional network layers to reduce latency and increase data transfer speeds, which improves efficiency for data-intensive tasks.

\subsection{SuperSockets}
SuperSockets is Dolphin’s high-performance socket API extension designed to offer an easy extension for applications that rely on traditional TCP/IP sockets. SuperSockets enable faster speeds while still using familiar socket interfaces since the data is transfered using RDMA. By bypassing much of the operating system overhead associated with traditional network stacks, SuperSockets facilitate direct, high-speed communication between nodes with minimal latency, taking advantage of RDMA and PCIe capabilities. This results in  performance improvements for distributed applications and is  beneficial for data-intensive workloads in HPC.

\subsection{SISCI}
SISCI (Software Infrastructure for Shared-Memory Cluster Interconnect) provides an API for applications needing direct memory access and memory sharing across multiple nodes. SISCI supports high-level abstractions for memory allocation, mapping, and synchronization. By leveraging PCIe’s peer-to-peer capabilities, SISCI enables processes on different nodes to access and manipulate each other's memory regions without CPU involvement, reducing latency and increasing throughput. SISCI also have PIO support.

\subsection{Dolphin IPoPCIe}
Dolphin’s IPoPCIe driver provides a transparent way to send and receive network traffic over PCI Express instead of Ethernet. By combining Programmed I/O (PIO) with RDMA transfers, it reduces system overhead and achieves significantly higher bandwidth and lower latency than standard Ethernet solutions. IPoPCIe still presents a TCP/IP interface to the operating system and applications. . However, IPoPCIe typically reduces latency by a factor of 5–6 compared to 10G Ethernet, while also delivering much higher data throughput, often over 50 Gbps in a standard x8 Gen3 PCIe configuration without requiring changes to user application. \cite{dolphin_fast_tcp_udp_ip}