\chapter{Background}
This chapter provides the technical background necessary to understand the systems and technologies used in this work. It starts with a brief overview of High-Performance Computing (HPC) and the I/O challenges these systems face. Next, it introduces the architecture and core features of BeeGFS. Finally, it explains key technologies relevant to this project, including RDMA, PCIe, and software solutions from Dolphin Interconnect, which are used in this work.

\section{History of HPC}
High-Performance Computing has a long history of driving advancements in science and technology. It began in the mid-20th century with the development of early computing systems such as the ENIAC in 1945, which was designed to handle complex calculations that were previously impossible. However, the introduction of the CDC 6600 in 1964, designed by Seymour Cray, is regarded as the birth of modern supercomputing.\cite{narayan2009supercomputers} This machine introduced parallel processing and vector computation, both key concepts that have shaped the evolution of HPC. 

Supercomputing continued to advance throughout the 1970s and 1980s, with systems like the Cray-1 in 1976 bringing higher processing speeds and improved efficiency through vector processing. These developments were crucial for fields such as physics, weather forecasting, and large-scale simulations, which required enormous amounts of computational power. The 1990s saw further innovation with the rise of massively parallel processing (MPP) systems, which used thousands of processors to perform calculations simultaneously. During this time, the Beowulf cluster architecture also emerged, enabling more affordable and scalable supercomputing by using standard hardware components.

In the 21st century, HPC has continued to grow in importance, with systems reaching petascale performance, such as \allowbreak IBM’s Roadrunner in 2008, capable of over one quadrillion calculations per second (1 petaflop). Today, the field is moving towards exascale computing, with new systems like Frontier aiming to exceed one exaflop ($10^{18}$ FLOPS).\cite{top500_2024} These advancements enable increasingly complex scientific research, including large-scale data analysis, climate modeling, and artificial intelligence.

As HPC systems evolve, improving individual components becomes essential to meet rising computational demands. Technologies like RDMA offer significant performance gains by enabling direct memory access between systems, reducing CPU involvement and improving data transfer speeds. 

\section{I/O as a bottleneck}

In HPC systems, I/O performance is critical to ensure that applications can efficiently read and write large volumes of data. However, I/O often becomes a significant bottleneck, limiting the overall performance of applications.\cite{9355272}

One of the main causes of I/O bottlenecks in HPC systems is the centralized or sequential data access patterns common in traditional file systems. These file systems are not designed to support many processes accessing data simultaneously. In HPC clusters, hundreds of compute nodes may try to read or write data simultaneously, which can overwhelm standard storage systems and create contention. This issue becomes more visible in data-intensive workloads, where frequent access to large datasets increases pressure on the storage system and reduces performance.

To address this, parallel file systems are used in HPC systems. File systems like BeeGFS, Lustre, and GPFS are designed to distribute data across multiple storage servers. This allows many processes to simultaneously read from or write to different parts of a file or dataset. Parallel file systems use data striping, where files are split into chunks and stored across several servers. This means that data can be accessed in parallel, which improves throughput and reduces the chance of any single server becoming a bottleneck.

In addition, parallel file systems separate metadata from data. Meta data, such as file names and directory structures, is handled by dedicated servers. This separation reduces delays caused by metadata operations, especially when many processes create, open, or list files simultaneously.

By distributing data and metadata across multiple servers and allowing parallel access, these file systems help avoid the limitations of traditional systems and improve I/O performance in HPC workloads.

\section{BeeGFS}

BeeGFS, earlier known as Fraunhofer Parallel File System, is a parallel file system designed for use in HPC systems. Initially developed by the Fraunhofer Institute for Industrial Mathematics in Germany, BeeGFS is built to scale out horizontally across many servers or nodes. This architecture allows BeeGFS to handle heavy workloads by spreading I/O operations across the storage nodes. The file system is mounted on top of an existing one, often XFS.

One of the key features of BeeGFS is the separation of file data and metadata, enabling efficient handling of file system operations like lookups, renames, and access permissions. This division means that metadata operations don’t interfere with data transfers, resulting in faster file operations and improved overall system performance. BeeGFS also includes monitoring, quotas/groups, and data mirroring. Additionally, BeeGFS integrates easily with existing infrastructure and supports standard POSIX file system rules.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/img/BeeGFS_big.png}
    \caption[BeeGFS Logo]{BeeGFS Logo. Used with permission from ThinkParQ.}
    \label{fig:beegfs_logo}
\end{figure}

\subsection{Architecture}

The BeeGFS architecture is composed of four main services. The storage service is in charge of handling the actual user file data across the distributed system. The metadata service manages details such as file permissions and striping configurations—the management service functions as a central registry and governing system for the other services. The client service allows the file system to be mounted, making the stored data accessible to users. Beyond these, BeeGFS also offers optional monitoring and event listening services.

\subsubsection{Storage Service}

\vspace{-1.2em}

The storage service in BeeGFS is responsible for storing the actual user file data, which is distributed across multiple storage targets in the form of chunk files. It follows a scale-out architecture, meaning multiple storage service instances can be deployed to increase both capacity and performance of the file system. Each storage service can manage multiple storage targets, allowing it to distribute data across several storage volumes. These targets often use the XFS file system because it scales well with RAID arrays and typically offers higher sustained write throughput on fast storage than other file systems. However, BeeGFS is compatible with any local POSIX-compliant file system.

Files are striped across multiple targets to balance load and improve throughput. The stripe pattern, defined at file creation by the metadata service, determines the chunk size and number of targets per file. This pattern can be configured per directory or file. See Figure \ref{fig:stripe} for a visual representation of how different striping configurations work over multiple targets. Chunk files are created lazily, meaning they are only allocated on a storage target when a client writes data. Their size matches the amount of data written, so no space is wasted on empty or partially filled chunks. Target selection for new files is randomized by default to ensure balanced usage across the system, but other selection strategies can be configured for specific needs.


\subsubsection{Metadata Service}

\vspace{-1.2em}

The BeeGFS metadata service manages file system metadata such as directory structures, file ownership, permissions, and the mapping of user files to their corresponding storage targets. When a client opens a file, the metadata service provides the stripe pattern indicating where the file's data is located, but it is not involved in read or write operations during the file’s use.

Like the storage service, the metadata service follows a scale-out design. Multiple metadata servers can be deployed, each responsible for a distinct portion of the global namespace. Additional metadata servers can be added at any time.

Each metadata server uses one dedicated metadata target for storage. Rather than storing all metadata in a centralized database, BeeGFS creates a separate metadata file for each user-created file. This approach reduces the risk of data corruption and simplifies metadata management.

Metadata targets typically run over ext4 file systems as this has yielded the best performance \cite{ext4}. Metadata overhead is minimal and scales linearly with the number of files. For example, 500 GB of metadata storage supports approximately 150 million files.

To further reduce latency and improve system responsiveness, BeeGFS benefits from fast CPU cores, as metadata operations are often latency-sensitive.

\subsubsection{Management Service}

\vspace{-1.2em}

The management service is the central coordination point for all other BeeGFS components, including metadata, storage, and client services. There is only one instance of this service. It is lightweight and does not handle user data or affect system performance directly, so it is typically deployed on a shared system rather than a dedicated machine.

Its primary role is maintaining a registry of all active services in the file system and monitoring their availability. As such, it is the first service to be configured when setting up a new BeeGFS installation.

\subsubsection{Client Service}

\vspace{-1.2em}

The BeeGFS client service integrates directly with the Linux kernel’s virtual file system interface. It includes a kernel module automatically compiled to match the running kernel, requiring no manual intervention during kernel or client updates.

In addition to the kernel module, a user-space daemon supports the client by handling DNS resolution and logging. The client mounts BeeGFS file systems based on entries in \texttt{beegfs-mounts.conf}, rather than the traditional \texttt{/etc/fstab} method. This approach simplifies service management and supports automatic module recompilation after system updates.

\subsection{Functionality}

\subsubsection{File Striping}

\vspace{-1.2em}

File striping is a key component of BeeGFS. It is a mechanism that distributes a file across multiple storage targets. This process divides a file into smaller chunks, which are then spread over the storage targets. The command line tools can adjust the striping pattern, number of targets, and chunk size. File striping can be combined with mirroring to achieve better performance and redundancy.

\begin{figure}[H]
    \centering
    \input{fig/tikz/stripe.tex}
    \caption[Striping \& Mirroring in BeeGFS]{File data is distributed across multiple storage targets in chunks. In a buddy-mirrored setup, each chunk is duplicated across two storage nodes. A storage node can store both unmirrored chunks and mirrored chunks if part of a buddy group. In this figure, you can see examples of the different storage methods.}
    \label{fig:stripe}
\end{figure}

\subsubsection{Mirroring}

\vspace{-1.2em}

BeeGFS supports mirroring for both file data and metadata, ensuring redundancy and high availability. Mirroring works by using buddy groups, where pairs of storage targets or metadata servers replicate the data. One server in the pair acts as the primary, handling all modifications while synchronizing data with the secondary. If the primary server fails, the system automatically promotes the secondary to primary after a short delay, keeping the data accessible. While mirroring protects against hardware failures, it does not replace backups, as it cannot recover files lost due to user mistakes or overwrites.

\subsection{Existing Communication Mechanisms}

BeeGFS supports two main communication methods: TCP over Ethernet (traditional socket-based) and RDMA. RDMA is supported over InfiniBand, RoCE (RDMA over Converged Ethernet), and Omni-Path. RDMA functionality in BeeGFS is implemented using the \textit{ibverbs} API from the Open Fabrics Enterprise Distribution (OFED).

Network interfaces used for communication are specified in a configuration file. If RDMA is enabled, BeeGFS may still use Ethernet for non-performance-critical messages, such as communication with the management service.

By default, BeeGFS clients and servers connect using any available interface that supports TCP or RDMA. If the primary interface fails, connections automatically switch to an alternative. This behavior can be customized using the respective configuration file, which allows administrators to restrict or prioritize specific interfaces or IP ranges.

Performance can be further improved through network tuning. This includes configuring window scaling, buffer sizes, and enabling timestamps for TCP. Jumbo frames can also be enabled on Ethernet interfaces to reduce overhead. For RDMA, tuning options include adjusting the number and size of buffers per connection, the fragmentation size of each buffer, and the number of parallel connections to other nodes.

\subsection{BeeGFS I/O Workflow}
When a client performs I/O in BeeGFS, it contacts the metadata server to retrieve file attributes and the chunk layout. Afterward, it communicates directly with the appropriate storage servers to read or write data. This design enables parallel data transfers.

BeeGFS has two client-side caching modes. The default mode is buffered caching, which uses fixed-size buffers for reading ahead and writing data back. This works well for streaming workloads and is required if the client and server run on the same machine. The second mode is native caching, which uses the Linux kernel's page cache. This allows more flexible and larger caching, which can help workloads that mostly use memory. However, the behavior can vary depending on the Linux kernel version. Different caching modes can be assigned to specific directories by mounting them with separate client configuration files.

\subsection{Prior Performance Evaluations of BeeGFS}

Recent studies have evaluated BeeGFS's performance across diverse HPC and deep learning (DL) workloads, highlighting its performance.

A study conducted by Graid Technology Inc. evaluated BeeGFS performance using NVMe SSDs and a high-speed 400G Ethernet network, comprising four 100G Ethernet links per storage node \cite{Graid2023}. The setup included two storage nodes, each with 16 SSDs managed by a SupremeRAID SR-1010 controller. Benchmark results showed read speeds reaching 130 GB/s and write speeds of 70 GB/s, highlighting BeeGFS's ability to deliver high throughput for data-intensive applications requiring high bandwidth.

Another study by Dell Technologies tested BeeGFS on PowerEdge servers with large HDD arrays, providing 2.69 PB of storage, connected via an InfiniBand HDR network \cite{Dell2023}. Benchmarks demonstrated read and write speeds of approximately 30 GB/s and up to 33,000 operations per second for random workloads, showcasing BeeGFS's reliability and performance in large-scale storage environments.

A third study explored BeeGFS for deep learning workloads on a mixed SSD and HDD setup, connected via a QLogic InfiniBand QDR network \cite{Chowdhury2019}. It achieved read speeds up to 16 GB/s for standard tasks and managed DL data effectively, though performance dipped with small, random file reads. Hierarchical data organization significantly enhanced outcomes.

These studies show that BeeGFS works well for both HPC and DL tasks but needs a configured setup to match specific needs.

\section{Data Transfer Mechanisms}

\subsection{Programmed Input/Output}

Programmed Input/Output (PIO) is a data transfer mechanism where the CPU directly controls data movement between memory and peripheral devices. In PIO mode, the CPU actively issues read or write instructions to device-specific registers, transferring data one word or byte at a time. Each transfer requires the CPU to poll the device status, ensure readiness, and execute the appropriate memory or I/O instructions, resulting in a tightly controlled but CPU-intensive process.

Because the CPU is involved in every data transfer step, PIO introduces significant processing overhead. During PIO operations, the CPU cannot perform other tasks, which reduces overall system efficiency, particularly when large volumes of data must be moved. Although PIO can offer lower latency for small, simple memory operations compared to setting up a DMA transfer, it is inefficient for handling large data transfers, where methods like Direct Memory Access (DMA) provide better throughput by offloading the work from the CPU. Despite its inefficiencies, PIO remains useful when data transfers are infrequent, small, or when system simplicity and hardware cost are prioritized.

\subsection{Direct Memory Access}

DMA is a mechanism in computer architecture that enables peripheral devices to read or write to main memory independently of the CPU. By offloading data transfer operations to a dedicated DMA controller, the CPU is relieved from managing low-level memory transactions, allowing it to focus on computation-intensive tasks. This results in improved system efficiency, particularly in workloads that require frequent or large-scale data movement.

A dedicated hardware component called the DMA controller manages the data transfer. While the CPU initiates the operation by specifying the source, destination, size, and direction, the actual data transfer occurs directly between the device and main memory, bypassing the CPU entirely. The DMA controller manages the transfer by issuing the necessary control signals on the system bus, but it does not move the data payload. Once the transfer is complete, the DMA controller typically signals the CPU via an interrupt. This division of responsibilities enables high-throughput, low-latency data movement with minimal CPU intervention.

While DMA significantly improves system performance by offloading data transfers from the CPU, it introduces a higher setup overhead than PIO. In PIO, the CPU manually manages each data movement between memory and peripheral devices, consuming valuable processing cycles and making it inefficient for large or frequent transfers. However, PIO can be preferable for short or infrequent transfers, as it avoids the configuration overhead associated with initiating a DMA operation.

The total transfer time for PIO and DMA can be expressed as follows, where $N$ is the number of bytes, $T$  is the total time, and $t$ is the time per byte transferred:
\[
T_{\text{PIO}} = N \times t_{\text{PIO}}
\]
\vspace{-2em}
\[
T_{\text{DMA}} = t_{\text{setup}} + (N \times t_{\text{DMA}})
\]
DMA becomes faster than PIO when:
\[
T_{\text{DMA}} < T_{\text{PIO}}
\]
which simplifies to:
\[
N > \frac{t_{\text{setup}}}{t_{\text{PIO}} - t_{\text{DMA}}}
\]
DMA is advantageous when the amount of data to transfer exceeds the threshold determined by the setup overhead and the per-unit transfer time difference between PIO and DMA.

Figure~\ref{fig:dmapio} illustrates the conceptual difference between PIO and DMA, highlighting how the CPU is involved directly in PIO, whereas in DMA, the CPU initiates the transfer but the actual data movement is handled independently by the DMA controller.

\begin{figure}[H]
    \centering
    \input{fig/tikz/dmapio}
    \caption{PIO vs. DMA}
    \label{fig:dmapio}
\end{figure}

RDMA, or Remote DMA, extends this core concept across networked systems.

\subsection{Remote Direct Memory Access}

Remote Direct Memory Access (RDMA) builds on the concept of Direct Memory Access (DMA) by extending it across networked systems. RDMA enables one computer to directly read from or write to the memory of another, without involving the remote CPU, operating system, or network stack. This results in lower latency, higher throughput, and reduced CPU overhead compared to traditional socket-based communication, which relies on system calls, context switches, and kernel-level processing.

RDMA transfers data directly between memory regions on different hosts, bypassing the software stack entirely during the transfer phase. This is especially beneficial in high-performance computing (HPC) environments, where networks are typically well-managed and predictable. In such settings, RDMA can skip many of the features built into TCP/IP, such as congestion control and retransmission.

RDMA operations are handled by RDMA-capable network interface cards (RNICs), which implement the necessary protocol logic in hardware. These RNICs take care of tasks like connection management, packet ordering, and acknowledgments, allowing the host CPU to remain idle once the transfer has been initiated. However, as with DMA, there is still some setup overhead when starting a transfer and PIO may be more suitable for short transfers.

There are three main types of RDMA operations: write, read, and send/receive. Write and read are one-sided operations, meaning the initiator can access the remote memory without the target CPU’s active involvement. Send/receive, on the other hand, is two-sided and requires both endpoints to participate. One-sided operations are key to RDMA’s efficiency, as they reduce synchronization and processing overhead.

RDMA supports multiple transport types depending on application requirements. Reliable Connection (RC) is the most commonly used, which ensures reliable, ordered delivery between two endpoints. Other options include Unreliable Connection (UC) and Unreliable Datagram (UD), which trade reliability for lower overhead or support for multicast.

RDMA is a generic concept that can run over various underlying technologies. InfiniBand and RDMA over Converged Ethernet (RoCE) are two of the most widely used implementations. InfiniBand is a dedicated high-speed interconnect often used in HPC clusters, while RoCE allows RDMA traffic to run over standard Ethernet networks. Each approach has different requirements.

Despite its performance benefits, RDMA introduces added complexity for developers. Applications must explicitly manage resources such as queue pairs, completion queues, and synchronization mechanisms. One essential step is memory registration. Before a memory region can be used in RDMA operations, it must be registered with the local RNIC. This process pins the memory to prevent swapping, sets up descriptors with access permissions and address mappings, and assigns access keys.

Memory registration is relatively expensive, so applications reuse registered buffers to reduce overhead. Some advanced RNICs support features like memory windows and on-demand paging to help minimize registration costs.

RDMA communication is organized using queue pairs, each consisting of a send and receive queue. Applications post work requests to these queues to initiate data transfers. These operations are handled entirely in hardware by the RNIC, enabling fast, low-latency communication between systems.

Figure~\ref{fig:rdma} illustrates the key difference between TCP/IP and RDMA. RDMA bypasses the operating system and skips the buffer copies.

\begin{figure}[H]
    \centering
    \input{fig/tikz/rdma}
    \caption[RDMA vs TCP]{TCP/IP networking requires multiple buffer copies and kernel processing, increasing latency and CPU use. RDMA transfers data directly between memory, reducing both.}
    \label{fig:rdma}
\end{figure}

Traditional networking protocols like TCP/IP require data to pass through the application, operating system, and network stack before reaching the network hardware. This process involves several memory copies and frequent transitions between user and kernel space, increasing latency and CPU usage.

RDMA avoids these overheads through zero-copy transfers. Once memory is registered, data can be moved directly from local to remote memory, without intermediate buffer copies or CPU involvement.

The key differences between TCP/IP and RDMA are summarized in Table~\ref{tab:rdma_vs_tcp}.

\begin{table}[h]
    \centering
    \caption{RDMA vs. TCP/IP}
    \label{tab:rdma_vs_tcp}
    \begin{tabularx}{\textwidth}{|l|X|X|}
        \hline
        \textbf{Feature} & \textbf{RDMA} & \textbf{TCP/IP} \\
        \hline
        CPU Involvement & Minimal & High \\
        Memory Copies   & Zero-copy & Multiple copies \\
        Kernel Bypass   & Yes & No \\
        Latency         & Low & Moderate to High \\
        Throughput      & High & Moderate \\
        \hline
    \end{tabularx}
\end{table}

\subsection{InfiniBand}

Originally developed by an industry consortium in the early 2000s, InfiniBand has become one of the most prominent technologies for high-speed, low-latency interconnects in data centers and high-performance computing. Mellanox Technologies, later acquired by NVIDIA, emerged as a key player in the InfiniBand ecosystem, delivering both hardware and software that helped drive adoption. InfiniBand is a high-speed interconnect architecture designed to support low-latency and high throughput. It operates over a switched fabric network using point-to-point links, typically arranged in fat-tree or similar topologies.

InfiniBand supports both reliable and unreliable transport modes, with hardware-managed packet ordering, flow control, and retransmission. These features are implemented entirely in the Host Channel Adapter (HCA) and switches, enabling deterministic performance under load.

The protocol supports multiple virtual lanes on a single physical link, each with independent credit-based flow control. This reduces head-of-line blocking and enables lossless communication by preventing buffer overflows before they occur.

Modern InfiniBand implementations, such as HDR (200 Gbps) and NDR (400 Gbps), use wide links with multiple lanes to achieve high full-duplex bandwidth. These characteristics make InfiniBand well-suited for environments demanding both high throughput and low latency.

\section{PCI Express}

Peripheral Component Interconnect Express (PCIe) is a high-speed, scalable interface standard that plays a central role in modern computer architecture. It facilitates communication between the CPU and peripheral devices such as graphics cards, storage drives, and network adapters, replacing interfaces like PCI and AGP. Thanks to its low latency and high bandwidth, PCIe has become vital in various computing environments, from consumer-grade systems to high-performance data centers.

Connecting two separate systems over PCIe is challenging because PCIe was originally designed as a local I/O bus within a single system, not between systems. It relies on tightly coupled memory access and direct hardware control, extending it across systems is non-trivial. However, it is possible to make this work with the right hardware and software support, such as special adapters and custom drivers. This setup can take full advantage of PCIe’s low latency and high bandwidth when done correctly. This section will explore core PCIe functionality and how to connect two systems in practice.

\subsection{Physical Structure}

A PCIe link comprises one or more lanes, each consisting of two unidirectional differential signal pairs, one for transmitting and one for receiving. A single PCIe lane therefore provides full-duplex communication, and multiple lanes can be used to increase bandwidth, commonly in x1, x4, x8, or x16 configurations.

Each PCIe device connects to the root complex (typically integrated into the CPU itself or the chipset) through point-to-point links or a switch. The root complex is the component that connects the CPU and memory to the PCIe bus, managing communication between the CPU and PCIe devices. It also holds the master configuration table, which contains information about the host memory space accessible from each device.

Unlike shared bus architectures, PCIe enables dedicated bandwidth per device. The physical interface uses edge connectors on add-in cards and corresponding slots on the motherboard. Still, in interconnect applications, PCIe can be extended via specialized cables and NTBs to link multiple systems.

\subsection{Protocol Stack Layers}
Like the OSI model, PCIe follows a layered architecture composed of three layers: the Transaction Layer, the Data Link Layer, and the Physical Layer. The Transaction Layer manages data packets and communication protocols between devices. The Data Link Layer ensures reliable transmission with error checking, while the Physical Layer handles electrical signaling across the connection.

\begin{figure}[H]
    \centering
    \input{fig/tikz/pcie_stack}
    \caption[PCIe Protocol Stack]{Overview of the PCIe protocol stack layers}

    \label{fig:pcie_stack}
\end{figure}

The Transaction Layer adds a header and optional payload/ECRC to form a Transaction Layer Packet (TLP), while the Data Link Layer appends a sequence number and LCRC, and generates a Data Link Layer Packet (DLLP) for control. The Physical Layer frames TLPs and DLLPs with STP and END/EDB symbols for transmission.

\begin{figure}[H]
    \centering
    \input{fig/tikz/pcie_frame}
    \caption[PCIe Frame]{Structure of a PCIe Frame}

    \label{fig:pcie_frame}
\end{figure}

\subsubsection{Transaction Layer}

The Transaction Layer is responsible for creating and processing Transaction Layer Packets (TLPs), which are the main type of messages used to send data and commands between devices in a PCIe system. It manages request and response transactions between devices by formatting TLPs, which are then sent and received by lower layers. Each TLP typically consists of a header, an optional data payload (absent in messages like read requests, completions without data, or control messages), and may also include an End-to-End Cyclic Redundancy Check (ECRC), which is an optional checksum used for detecting errors between the source and destination. The header, which is either 12 or 16 bytes long depending on the type of transaction, contains important details such as the packet type (like memory read, memory write, or completion) and the destination address.

Request TLPs are sent by devices such as a CPU, root complex, or endpoint to perform actions like reading from memory, writing to memory, or accessing configuration space. When a response is needed, for example, in memory or configuration reads, the target device sends back a Completion TLP that includes the requested data. When a payload is included, it can be up to 4096 bytes in size and must be aligned on 32-bit boundaries. The maximum payload size is set by a control register in the device's configuration space. 

\subsubsection{Data Link Layer}

The Data Link Layer ensures reliable delivery of Transaction Layer Packets (TLPs) using an ACK/NAK protocol. Each TLP is assigned a sequence number and protected by a 32-bit Link Cyclic Redundancy Check (LCRC). If an error is detected, the receiver sends a NAK to request retransmission; otherwise, it replies with an ACK. These control messages, along with flow control and power management signals, are transmitted as Data Link Layer Packets (DLLPs), which operate independently of TLPs but share the same physical link.

Flow control is implemented using a credit-based system, where the sender tracks available buffer space at the receiver. Credits are managed separately for three classes of TLPs: posted (e.g., memory writes), non-posted (e.g., memory reads), and completion packets (responses to non-posted requests). This separation allows the sender to transmit different types of traffic without blocking or starving other traffic classes. Credit information is exchanged through DLLPs during link initialization and updated dynamically during operation.

\subsubsection{Physical Layer}

The Physical Layer handles the transmission and reception of raw bits across the PCIe link. It includes components like serializers, deserializers, and encoding logic that convert data between serial and parallel formats. In multi-lane configurations, data is striped across lanes at the byte or symbol level, depending on the PCIe generation, allowing higher throughput. The layer uses encoding schemes such as 8b/10b or 128b/130b, which support synchronization, clock recovery, and basic error detection.

Special control symbols such as STP (Start of TLP), SDP (Start of DLLP), END (End of Packet), and EDB (End with Error) are used to mark packet boundaries and indicate transmission errors. These symbols are not part of the payload and help the receiver correctly frame and process packets. Ordered sets, which are distinct from control symbols, are used during link initialization and training to negotiate link parameters like lane width, speed, and alignment. All types of packets, including control messages, interrupts, and data, share the same physical lanes and are distinguished by their framing and encoding.

\subsection{Configuration and Addressing}

In a PCIe device, the configuration space is a special set of registers used to control and identify the device. These registers are mapped to specific memory addresses in the system, allowing the CPU to access them during device initialization and configuration. The PCIe configuration space can be up to 4096 bytes and contains essential information such as the vendor ID, device ID, device class, and various status and control flags. Device drivers use this space to understand the capabilities of the device and to configure its operation. Operating systems typically provide APIs or system calls to safely access this space, such as \textit{lspci} and \textit{setpci} on Linux.

A key component of the PCIe configuration space is the set of Base Address Registers (BARs), starting at offset 0x10. Each device can define up to six BARs (BAR0–BAR5), which indicate the size and type of resource (memory or I/O) the device needs. During enumeration, the host allocates appropriate address ranges in the system’s physical address space and writes them into these BARs. Once set, these BARs map the device’s registers and on-board memory into the host’s address space. This allows the CPU or device driver to interact with the hardware via memory-mapped I/O (MMIO), simply by reading from or writing to memory addresses. Most modern PCIe devices rely exclusively on memory BARs, using either 32-bit or 64-bit addressing.

MMIO is the dominant mechanism for communication between software and PCIe devices. It allows devices to expose control registers, configuration options, and data buffers as part of the host’s memory space. Accesses to these mapped addresses translate into PCIe read and write transactions on the bus. When software performs a memory access to a device’s MMIO region, the host’s PCIe controller (typically part of the root complex) detects the access and translates it into a TLP.

\begin{listing}[H]
\begin{minted}[label={lst:pcie_code}]{c}
#define MMIO_BASE   0xF8000000
#define REG_OFFSET  0x10

volatile uint32_t *reg = (uint32_t *)(MMIO_BASE + REG_OFFSET);
uint32_t value = *reg;    // Triggers a PCIe Memory Read  TLP
*reg = 0xDEADBEEF;        // Triggers a PCIe Memory Write TLP
\end{minted}
\caption{PCIe Memory Access Example}
\end{listing}

PCIe bus enumeration is the process where the host system detects all connected devices, assigns them resources (such as memory regions via BARs), and configures their settings. Enumeration begins at the root complex and traverses downstream through any PCIe bridges to discover subordinate devices. PCIe bridges expand the bus and introduce a hierarchical structure, allowing for complex topologies. Devices are addressed using a tuple of bus number, device number, and function number. Endpoint devices use a Type 0 configuration space, while bridges use a Type 1 configuration space to support hierarchical enumeration.

Connecting two computers directly over PCIe introduces unique challenges. PCIe was designed for a single-root hierarchy, with a central host (the root complex) managing all configuration. In a direct host-to-host connection, no such root exists to perform enumeration, allocate resources, or configure BARs. This makes standard PCIe enumeration mechanisms unusable. Specialized solutions such as a Non-Transparent Bridge (NTB), are  required to enable communication between two hosts over PCIe without a central root complex.

\subsection{Non-Transparent Bridge}

In standard PCIe communication, a single root complex manages the PCIe fabric and memory space. However, when connecting two independent systems, each with its own root complex and memory domain, a direct (transparent) connection is not feasible due to the lack of a shared memory address space. This limitation creates the need for an NTB.

\begin{figure}[H]
    \centering
    \input{fig/tikz/ntb}
    \caption[NTB Address Translation]{NTB Address Translation}

    \label{fig:ntb}
\end{figure}

An NTB enables communication between multiple systems over PCIe while preserving system isolation. It acts as a PCIe endpoint from the perspective of each system’s root complex, allowing the systems to exchange data without merging their address spaces. Each system continues to operate its own operating system and memory management independently.

The NTB performs address translation on TLPs. When a TLP generated by one system arrives at the NTB, the memory address in the packet header is rewritten according to a configured translation window before being forwarded to the destination system. This modification is essential, as the original address would not be valid or meaningful in the remote system’s memory space.

Address translation is configured using BARs within the NTB’s Configuration and Status Registers (CSRs). These registers define which regions of local memory are exposed to the remote system, and how incoming memory requests are redirected. This allows shared memory regions to be established across systems in a controlled and isolated manner.

Additionally, NTBs often include doorbell registers and scratchpad registers to facilitate coordination. Doorbells can be used to trigger interrupts or signal events such as data availability, while scratchpads provide small, shared memory buffers for message passing or status flags.
See Figure~\ref{fig:adress_translation} for an illustration of the address translation process across an NTB. \cite{PLX_NTB_PCIe}

\begin{figure}[H]
    \centering
    \input{fig/tikz/address_translation}
    \caption[NTB Address Translation]{NTB Address Mapping}

    \label{fig:adress_translation}
\end{figure}

\section{Dolphin Interconnects}

Dolphin Interconnect Solutions is a technology company specializing in high-performance interconnect technologies, with a focus on low-latency and high-bandwidth communication in computing environments.

Since the early 1990s, Dolphin has contributed to the development and adoption of industry standards such as PCI, SCI (Scalable Coherent Interface), StarFabric, and PCIe. Through its innovations, Dolphin has helped shape the landscape of interconnect technologies for demanding industrial and research applications.

Dolphin offers a platform that combines PCIe-based hardware with its eXpressWare software suite (illustrated in Figure \ref{fig:dolphin_stack}), enabling the construction of robust, high-performance systems with efficient data transfer capabilities.\cite{dolphinics2025}

\begin{figure}[H]
    \centering
    \input{fig/tikz/dolphin_stack}
    \caption[eXpressWare Stack]{Overview of Dolphin’s eXpressWare software stack}

    \label{fig:dolphin_stack}
\end{figure}
\subsection{IPoPCIe}

IPoPCIe (also known as DIS IP) is Dolphin Interconnect's TCP/IP driver for PCIe. It enables data transmission over PCIe while maintaining compatibility with standard TCP/IP-based applications. IPoPCIe appears as a regular network interface to the operating system, making it indistinguishable from other TCP interfaces. As a result, no application-level modifications are needed to take advantage of the performance benefits it provides.

The driver combines PIO for small messages with RDMA for larger data transfers. IPoPCIe operates through the conventional TCP/IP stack, which introduces software overhead. However, it still benefits from the much higher bandwidth and lower latency of PCIe. These hardware advantages result in improved performance, particularly in latency-sensitive applications. In typical scenarios, IPoPCIe can offer up to six times lower latency compared to 10G Ethernet. \cite{dolphin_fast_tcp_udp_ip}

\subsection{SuperSockets}

SuperSockets is a low latency implemention of the Berkeley Sockets interface using RDMA and PIO over PCIe \cite{seifert2004scisocket}. The goal is to bypass the TCP/IP stack to minimase software overhead. It introduces a custom address family called \texttt{AF\_SSOCKS}, which uses the same structures as and is compliant with \texttt{AF\_INET}, making it compatible with existing socket logic. This means that applications can make use of SuperSockets with minimal modifications. The index of \texttt{AF\_SSOCKS} is 27 by default but can be customized.

There are two different implementations, one in user-space and one in kernel-space. This refers to where the underlying functionality is implemented, not how it is used. For example, even if a kernel space SuperSocket is set up from user space, it is still considered a kernel space SuperSocket. User space SuperSockets generally provide better performance than traditional kernel space SuperSockets. They are built on the SISCI API and aim to offload as much functionality as possible in user space, to avoid costly context switches into kernel space.

The data transport and associated protocols are handled by a loadable Linux kernel device driver, \texttt{dis\_ssocks}. SuperSockets are built on top of the SISCI API and the kernel space interface called GENIF. See Figure \ref{fig:ssocks_stack} for how the logic is tied together.

One of the advantages of SuperSockets is that they can be implemented in user-space applications without them being recompiled. Most applications are linked dynamically to the C standard library, which provides the sockets interface. However, by utilizing the preload mechanism, we can instruct the dynamic linker to load a custom SuperSocket library before the standard libraries.

In practice, this means setting the environment variable LD\_PRELOAD to \path{libksupersockets.so} (or \path{libusupersockets.so} for user space SuperSockets) before starting the application. Dolphin Interconnects also provides a script to do this. User-space supersockets must be enabled this way and works by overloading socket function calls. Not every socket related fucntion is supported yet meaning not all applications can make use of it.

For kernel modules, only kernel-space SuperSockets can be used. Support must be implemented directly in the source code and recompiled.

SuperSockets transmits data over PCIe, but it does not appear to the operating system as a standard network interface. However, it still uses the same IP address typically associated with a regular TCP/Ethernet connection. SuperSockets also supports transparent failover to Ethernet if the PCIe connection is lost, and fail-forward functionality when the connection is restored.
 
\begin{figure}[H]
    \centering
    \input{fig/tikz/ssocks_stack}
    \caption[SuperSockets Architecture]{SuperSockets Architecture}

    \label{fig:ssocks_stack}
\end{figure}

\subsection{SISCI API}
The Software Infrastructure for Shared-Memory Cluster Interconnect (SISCI) API is a software framework designed to facilitate high-performance data sharing between interconnected systems. Developed specifically for use with Dolphin’s PCIe Non-Transparent Bridges (NTBs), SISCI enables RDMA exchanes between systems. Compared to SuperSockets, SISCI offers a more fine grained control over RDMA functionality.

The API operates by first establishing a virtual device, which serves as the primary interface for managing the underlying hardware. Developers then define memory segments, which are allocated blocks of memory that can be shared across connected systems. These segments are mapped into the system’s memory space, allowing remote machines to read from or write to them. SISCI supports both PIO and RDMA, and it is up to developers to define when to use it.

Additionally, SISCI provides mechanisms for sending and receiving application\allowbreak-level interrupts, enabling signaling between nodes to indicate events or data availability. Operations are managed through resources like memory segments and DMA queues, each controlled via designated handles.

\subsection{Underlying Principles}
SCILib is a thin wrapper library built on top of the SISCI API. SCILib offers unidirectional message queues and is the base of SuperSockets and IPoPCIe. SCILib has also been ported to kernel space using the
GENIF interface to support Kernel-space SuperSockets\cite{seifert2004scisocket}.

SCILib selects between three data transfer methods based on the payload size, using different combinations of PIO and RDMA. The three message types are:

\begin{itemize}
  \item \textbf{INLINE} – for payloads up to 116 bytes.  
        Both the control information and data fit into a single 128-byte PCIe write, achieving sub-3~µs latency.
  \item \textbf{SHORT} – for payloads of a few kilobytes.  
        The library first copies the data into a local buffer and then transfers it to the peer using RDMA.
  \item \textbf{LONG} – for larger payloads.  
        The sender performs an RDMA write directly from the caller’s buffer without any intermediate copy.
\end{itemize}

\subsection{Dolphin Hardware}
Dolphin provides a range of PCIe-based interconnect hardware designed for use with their software stack. Their product line includes NTBs, PCIe switches, and custom PCIe cables. The hardware used in this thesis is detailed in the results chapter.

\section{Storage Benchmarks}

\subsection{FIO}

Flexible I/O Tester (FIO) is a widely used disk and file I/O benchmarking tool used in research and industry due to its flexibility and broad range of features. It can generate different types of I/O workloads, such as sequential, random, and mixed read/write operations, making it suitable for testing various storage performance scenarios.

FIO provides fine-grained control over access patterns, block sizes, I/O depth, and other parameters. It also supports multi-threaded and multi-process execution, making it well-suited for simulating real-world parallel I/O workloads.

Another important advantage of FIO is its automation capabilities. It can run multiple jobs sequentially or in parallel, reducing manual work and ensuring consistency during benchmarking. This makes it especially useful for large-scale testing and repeated experiments, where maintaining uniform test conditions is essential.

FIO provides detailed performance metrics, including throughput, IOPS, latency, and CPU utilization. It also supports multiple I/O engines, such as \textit{sync}, \textit{mmap}, and \textit{libaio}, with \textit{libaio} being the primary engine used in this thesis.

FIO is used for some of the benchmarking in this work. The benchmark results from FIO are generated in JSON format. Automated scripts transfers the results from the testbed to a personal Apple Mac computer, where they are processed and visualized using Python scripts with Matplotlib. The results, plotting tools, and helper scripts for specifying plot parameters and generating multiple visualizations are all available in the attached GitHub repository.

\subsection{MLPerf Storage}
Currently empty.
