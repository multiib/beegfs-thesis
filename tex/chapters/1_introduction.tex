\chapter{Introduction}

\section{Motivation}
In an era where scientific discovery and technological innovation are accelerating at unprecedented rates, high-performance computing (HPC) has become the backbone of global advancement. From simulating complex molecular interactions to modeling climate change, HPC systems allow us to solve problems that were once beyond reach. As the need for faster and larger computations grows, the performance of the storage systems behind these workloads becomes more important than ever. In many modern HPC systems, input/output (I/O) is a major bottleneck. While processors and memory have improved quickly, storage systems often struggle to keep up \cite{nersc_storage_2020} \cite{9355272}. As a result, applications can spend a lot of time waiting for data to load or save, which slows everything down.

A common reason for this bottleneck is the way data is accessed. In large-scale HPC clusters, hundreds of compute nodes may try to access the same data simultaneously. Traditional filesystems were not built for this kind of workload, which leads to contention and delays. To solve this problem, many HPC environments use parallel filesystems like BeeGFS. These systems improve performance by spreading data across multiple servers and separating file metadata from the data. \cite{6061137} This separation allows metadata to be processed more quickly, which is important because even small delays in metadata access can hold up large workloads.

Another way to improve performance is by speeding up how data moves between systems. Remote Direct Memory Access (RDMA) is a technology that allows computers to read and write each other’s memory directly without involving the operating system or CPU. This reduces latency, increases throughput, and lowers CPU usage. Currently, RDMA is mainly used with InfiniBand, a high-speed network technology, but not all systems support InfiniBand or have it available.

Dolphin Interconnect Solutions offers an alternative by enabling RDMA-style communication over PCI Express (PCIe), which is widely used inside computers. Their technologies, such as SuperSockets and SISCI, allow systems to communicate with high speed and low latency over PCIe using dedicated software and hardware.

This thesis explores the integration of Dolphin’s PCIe-based communication technologies into BeeGFS. Doing so aims to provide an alternative to InfiniBand, allowing RDMA benefits on systems that already use PCIe. The goal is to improve data and metadata performance in BeeGFS, reduce latency, and better utilize the hardware. Even small gains in performance can lead to major improvements at scale, making this integration a valuable step toward more efficient HPC systems.

\section{Research Objectives}
\subsection{Main Problem Statement}
BeeGFS, a scalable parallel filesystem, leverages RDMA over InfiniBand for high-speed data transfers but lacks native support for RDMA-style communication over PCIe, limiting its flexibility on common hardware.

\begin{center}
    \textbf{\textit{The main goal of this thesis is to add SuperSocket support to BeeGFS over Dolphin's PCIe NTB fabric and compare the performance with existing transport methods.}}
\end{center}

\subsection{Specific Objectives}
\begin{itemize}
    \item \textbf{Integration of SuperSockets into BeeGFS}: Modify the BeeGFS codebase to support SuperSockets, enabling RDMA over PCIe Non-Transparent Bridges \allowbreak(NTB).
    \item \textbf{Performance Evaluation}: Assess the impact of SuperSockets integration on BeeGFS performance metrics such as latency, bandwidth, and throughput.
    \item \textbf{Feasibility Study}: Investigate the technical effort required to integrate Dolphin’s PCIe NTB communication stack into BeeGFS, including necessary source code modifications and compatibility considerations within the existing architecture.
    
    \item \textbf{Comparative Analysis}: Benchmark the performance of BeeGFS using SuperSockets and IPoPCIe against its performance over InfiniBand and standard Ethernet, focusing on latency, throughput, and scalability.

\end{itemize}

\subsection{Limitations}

The integration discussed in this thesis is not designed for production use, which may lack the robustness and fault-handling capabilities required for such environments. Additionally, scalability testing is limited by the available hardware, so the results may not fully represent performance in large-scale HPC systems. Aspects like security and multi-user access scenarios are beyond the scope of this thesis.
\section{Main Contributions}
To be written.

\section{Methodology}

This thesis uses a framework from the ACM Task Force on the Core of Computer Science \cite{Denning1989}, which divides computing into three main areas: theory, abstraction, and design.

First, the theory involves studying how BeeGFS currently works with Ethernet and InfiniBand, and checking if SuperSockets can be used similarly with PCIe NTBs.

Next, abstraction refers to how SuperSockets are expected to perform in BeeGFS, especially regarding latency and throughput, and preparing benchmarks to evaluate this.

Then, the design step includes changing the BeeGFS source code to add support for SuperSockets and IP over PCIe, followed by testing and fixing any issues. Finally, the performance of SuperSockets is compared to Ethernet, InfiniBand, and IP over PCIe using benchmarks. This approach helps combine analysis, testing, and hands-on changes to meet the thesis goals.

\section{Thesis Outline}

\subsection{Overview of the Thesis Structure}

This thesis is organized into six chapters to address the problem systematically. Chapter 1 introduces the motivation and problem statement. Chapter 2 provides a comprehensive background, covering the history of HPC, I/O bottlenecks, BeeGFS architecture, RDMA principles, PCIe fundamentals, and Dolphin Interconnect technologies, establishing the technical foundation for the work.

Chapter 3 describes the system design and implementation, focusing on the technical details on how SuperSockets was implemented into BeeGFS. Chapter 4 presents the results, detailing performance metrics across different setups and interfaces, offering a clear view of the raw outcomes.

Chapter 5 discusses these findings, analyzes their implications, and compares them to existing solutions, providing an interpretation of the results. Finally, Chapter 6 concludes the thesis with key insights and outlines directions for future work. Appendices and a bibliography provide additional resources and references to support the research.

\subsection{Notes on Approach and Conventions}

This thesis adopts a technical and experimental approach, prioritizing clarity and reproducibility in its presentation. Terminology follows standard HPC conventions (e.g., "RDMA" for Remote Direct Memory Access, "PCIe" for PCI Express), with abbreviations defined upon first use. Figures, tables, and code snippets are included to illustrate key concepts and results, with detailed data and scripts available in the accompanying repository for transparency. The content is intended to be approachable, requiring only a general understanding of computing concepts to follow the discussion.
